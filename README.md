# Terminal Autocomplete

A lightweight ML-powered CLI tool that autocompletes your terminal commands based on your past `.bash_history`. Built using PyTorch, LSTM, and a custom tokenizer. This project stemmed from the fact that at work I have to use a ton of commands (that I tend to forget)!

---

## ðŸš€ Features

- LSTM-based next-token prediction
- Tokenizer, batching, vocab, and training pipeline
- Interactive CLI with `typer`
- Beautiful `rich` table output
- Reusable model + vocab loading
- CUDA and CPU-compatible

---

## ðŸ“ Project Structure

``` bash
ml-terminal-autocomplete/
â”œâ”€â”€ model/
â”‚ â”œâ”€â”€ model_def.py # LSTM architecture
â”‚ â”œâ”€â”€ dataset.py # Tokenization + batching
â”‚ â”œâ”€â”€ train_model.py # Training loop
â”‚ â””â”€â”€ predict.py # Inference
â”œâ”€â”€ cli/
â”‚ â””â”€â”€ main.py # Interactive CLI tool
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ bash_history.txt # Raw data
â”‚ â”œâ”€â”€ tokenized_sequences.pkl
â”‚ â””â”€â”€ vocab.json
â”œâ”€â”€ saved_models/
â”‚ â””â”€â”€ best_model.pt # Saved trained model
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ðŸ› ï¸ Setup

```bash
# Clone the repo
git clone https://github.com/YOUR_USERNAME/ml-terminal-autocomplete.git
cd ml-terminal-autocomplete

# Set up virtual env (optional but recommended)
python -m venv venv
source venv/bin/activate   # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
```

## ðŸ§  Train the Model
```bash
python model/dataset.py        # Tokenize + save sequences
python model/train_model.py    # Train and save best model
```

## ðŸ§ª Run Interactive CLI
```bash
python -m main interactive start
```
Example:
```bash
>>> git ch
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Token        â”‚ Probability â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ checkout     â”‚ 0.8235      â”‚
â”‚ cherry-pick  â”‚ 0.0921      â”‚
â”‚ commit       â”‚ 0.0544      â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```
## ðŸ“„ License
[MIT License](LICENSE.txt)

## ðŸš§ Roadmap

This project is functional end-to-end, but there are several areas for improvement to increase usefulness, accuracy, and polish:

### Phase 1 â€“ Core Functionality
- [x] Load and preprocess a userâ€™s `bash_history`
- [x] Tokenize sequences (char-level)
- [x] Train an LSTM-based model to predict the next character
- [x] Build a prediction engine using the trained model
- [x] Implement a CLI with `typer` and styled output via `rich`
- [x] Allow interactive predictions from user-typed shell fragments

---

### Phase 2 â€“ Model Improvement (In Progress)
- [ ] Switch to **token-level modeling** (predict next full token, not character)
- [ ] Improve dataset quality (real-world command history, fewer `<UNK>` tokens)
- [ ] Add greedy or beam search to complete full word predictions from char-level model (if char-level is kept)
- [ ] Add more training data (2kâ€“5k lines) for better generalization
- [ ] Introduce dropout/regularization tuning for better performance
- [ ] Optionally try transformer-based architecture (`nn.Transformer`, `GPT2`, etc.)

---

### Phase 3 â€“ Evaluation & Testing
- [ ] Add train/val/test split (e.g., 80/10/10)
- [ ] Report metrics during training (e.g., loss, accuracy)
- [ ] Add test-time evaluation: given real partial commands, does the model predict the correct token?
- [ ] Add per-epoch logging and loss visualization (e.g., `matplotlib`, `tensorboard`, or simple CLI output)

---

### Phase 4 â€“ User Experience & Features
- [ ] Add shell-style **autocomplete** (TAB-key mimic, fuzzy match)
- [ ] Auto-complete full commands rather than just showing suggestions
- [ ] Add CLI option to show top-k completions as a single line or inline
- [ ] Create a `bash` or `zsh` plugin that calls the model for real-time shell autocompletion
- [ ] Make CLI installable via `pip` (`setup.py` or `pyproject.toml`)

---

### Phase 5 â€“ Deployment & Distribution
- [ ] Add `pip` install support (e.g., `pip install ml-terminal-autocomplete`)
- [ ] Dockerize for easy use anywhere
- [ ] Publish demo video/gif in README
- [ ] Add usage examples in the README with screenshots
